{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint 7 Chapter 6/7 Course Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Project description**\n",
    "\n",
    "  (from the project for the Statistical Data Analysis course). For this classification task, you need to develop a model that will pick the right plan. Since you‚Äôve already performed the data preprocessing step, you can move straight to creating the model.\n",
    "\n",
    "Develop a model with the highest possible _accuracy_. In this project, the threshold for _accuracy_ is 0.75. Check the _accuracy_ using the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">‚úîÔ∏è\n",
    "    \n",
    "\n",
    "__Reviewer's comment ‚Ññ1__\n",
    "An excellent practice is to describe the goal and main steps in your own words (a skill that will help a lot on a final project). It would be good to add the progress and purpose of the study."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# adding afther revicion V.1\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    f1_score, \n",
    "    roc_auc_score\n",
    ")\n",
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">‚úîÔ∏è\n",
    "    \n",
    "\n",
    "__Reviewer's comment ‚Ññ1__\n",
    "    \n",
    "Great, the libraries are loaded    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/datasets/users_behavior.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>calls</th>\n",
       "      <th>minutes</th>\n",
       "      <th>messages</th>\n",
       "      <th>mb_used</th>\n",
       "      <th>is_ultra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40.0</td>\n",
       "      <td>311.90</td>\n",
       "      <td>83.0</td>\n",
       "      <td>19915.42</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85.0</td>\n",
       "      <td>516.75</td>\n",
       "      <td>56.0</td>\n",
       "      <td>22696.96</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>77.0</td>\n",
       "      <td>467.66</td>\n",
       "      <td>86.0</td>\n",
       "      <td>21060.45</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>106.0</td>\n",
       "      <td>745.53</td>\n",
       "      <td>81.0</td>\n",
       "      <td>8437.39</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>66.0</td>\n",
       "      <td>418.74</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14502.75</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   calls  minutes  messages   mb_used  is_ultra\n",
       "0   40.0   311.90      83.0  19915.42         0\n",
       "1   85.0   516.75      56.0  22696.96         0\n",
       "2   77.0   467.66      86.0  21060.45         0\n",
       "3  106.0   745.53      81.0   8437.39         1\n",
       "4   66.0   418.74       1.0  14502.75         0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### view summary of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3214 entries, 0 to 3213\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   calls     3214 non-null   float64\n",
      " 1   minutes   3214 non-null   float64\n",
      " 2   messages  3214 non-null   float64\n",
      " 3   mb_used   3214 non-null   float64\n",
      " 4   is_ultra  3214 non-null   int64  \n",
      "dtypes: float64(4), int64(1)\n",
      "memory usage: 125.7 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# check duplicated rows\n",
    "print(df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check duplicated rows\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No find missing values and no row was duplicated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the dataset is split in two set features and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.drop(['is_ultra'], axis=1)\n",
    "target = df['is_ultra'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into separate training and test set\n",
    "\n",
    "Declare six variables and pass them the following:\n",
    "    features: features_train , features_valid and features_test;\n",
    "    target feature: target_train, target_valid and target_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_train = 0.8\n",
    "ratio_val = 0.2\n",
    "ratio_test = 0.2\n",
    "\n",
    "# Produces test split.\n",
    "remaining_train, features_test, remaining_target, target_test = train_test_split(\n",
    "    features, target, test_size=ratio_test,random_state=12345)\n",
    "\n",
    "# Adjusts val ratio, w.r.t. remaining dataset.\n",
    "ratio_remaining = 1 - ratio_test\n",
    "ratio_val_adjusted = ratio_val / ratio_remaining\n",
    "\n",
    "# Produces train and val splits.\n",
    "features_train, features_valid, target_train, target_valid = train_test_split(\n",
    "    remaining_train, remaining_target, test_size=ratio_val_adjusted,random_state=12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print number row and columns by each subset (train, validand test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1928, 4)\n",
      "(1928,)\n",
      "(643, 4)\n",
      "(643,)\n",
      "(643, 4)\n",
      "(643,)\n"
     ]
    }
   ],
   "source": [
    "print(features_train.shape)\n",
    "print(target_train.shape)\n",
    "print(features_valid.shape)\n",
    "print(target_valid.shape)\n",
    "print(features_test.shape)\n",
    "print(target_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">‚úîÔ∏è\n",
    "    \n",
    "\n",
    "__Reviewer's comment ‚Ññ1__\n",
    "\n",
    "1. It is good here, random_state is fixed. We have ensured reproducibility of the results of splitting the sample into training (training) / test / validation samples, so the subsamples will be identical in all subsequent runs of our code.\n",
    "    \n",
    "2. Fraction of train/valid/test sizes 3:1:1 is good.\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision tree classifier tuning Hyperparameters [max_depth]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth = 1 : 0.7387247278382582\n",
      "max_depth = 2 : 0.7573872472783826\n",
      "max_depth = 3 : 0.7651632970451011\n",
      "max_depth = 4 : 0.7636080870917574\n",
      "max_depth = 5 : 0.7589424572317263\n"
     ]
    }
   ],
   "source": [
    "for depth in range(1, 6):\n",
    "    model = DecisionTreeClassifier(random_state=12345, max_depth=depth)\n",
    "    model.fit(features_train, target_train)\n",
    "    predictions_valid = model.predict(features_valid)\n",
    "    print('max_depth =', depth, ': ', end='')\n",
    "    print(accuracy_score(target_valid, predictions_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree classifier \n",
    "\n",
    "tuning Hyperparameters [max_depth] with test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST - max_depth = 1 : 0.7480559875583204\n",
      "TEST - max_depth = 2 : 0.7838258164852255\n",
      "TEST - max_depth = 3 : 0.7869362363919129\n",
      "TEST - max_depth = 4 : 0.7869362363919129\n",
      "TEST - max_depth = 5 : 0.7884914463452566\n"
     ]
    }
   ],
   "source": [
    "for depth in range(1, 6):\n",
    "    model = DecisionTreeClassifier(random_state=12345, max_depth=depth)\n",
    "    model.fit(features_train, target_train)\n",
    "    predictions_test = model.predict(features_test)\n",
    "    print('TEST - max_depth =', depth, ': ', end='')\n",
    "    print(accuracy_score(target_test, predictions_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max-depth = 3 , will use for final calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.46158830531988904\n"
     ]
    }
   ],
   "source": [
    "model1 = DecisionTreeClassifier(random_state=12345, max_depth=3)\n",
    "model1.fit(features_train, target_train)\n",
    "predictions_test = model1.predict(features_test)\n",
    "mse = mean_squared_error(target_test, predictions_test)\n",
    "# < find the square root of MSE >\n",
    "rmse = mse ** 0.5\n",
    "print('RMSE:', rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the Model\n",
    "Compare the predicted labels with the true labels from the validation set or test set. Calculate the chosen performance metric using appropriate functions like accuracy_score(), precision_score(), recall_score(), f1_score(), or roc_auc_score()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7869362363919129\n",
      "Precision: 0.7521367521367521\n",
      "Recall: 0.4489795918367347\n",
      "F1-Score: 0.5623003194888179\n",
      "ROC AUC: 0.6920513171711639\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(target_test, predictions_test)\n",
    "precision = precision_score(target_test, predictions_test)\n",
    "recall = recall_score(target_test, predictions_test)\n",
    "f1 = f1_score(target_test, predictions_test)\n",
    "roc_auc = roc_auc_score(target_test, predictions_test)\n",
    " \n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1-Score: {f1}')\n",
    "print(f'ROC AUC: {roc_auc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of rmse is be of RMSE 0.46 indcated good fit the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fit_time': array([0.00578451, 0.00490141, 0.00518131, 0.00565577, 0.00487518]), 'score_time': array([0.0019803 , 0.03015614, 0.00223184, 0.00195837, 0.00189424]), 'test_score': array([0.79533679, 0.80829016, 0.80569948, 0.79220779, 0.82597403]), 'train_score': array([0.81582361, 0.80674449, 0.81322957, 0.81659106, 0.80816591])}\n"
     ]
    }
   ],
   "source": [
    "# Perform 5-fold cross-validation\n",
    "cv_results = cross_validate(model1, features_train, target_train, cv=5,return_train_score=True)\n",
    "\n",
    "# Print the results\n",
    "print(cv_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the best model on the validation set (n_estimators = 10): 0.7853810264385692\n"
     ]
    }
   ],
   "source": [
    "best_score = 0\n",
    "best_est = 0\n",
    "for est in range(1, 11):\n",
    "    model = RandomForestClassifier(random_state=54321, n_estimators=est)\n",
    "    model.fit(features_train, target_train)\n",
    "    score = model.score(features_valid, target_valid)\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_est = est\n",
    "\n",
    "print(\"Accuracy of the best model on the validation set (n_estimators = {}): {}\".format(best_est, best_score))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=10, random_state=54321)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RFC_model = RandomForestClassifier(random_state=54321, n_estimators=best_est)\n",
    "RFC_model.fit(features_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.4732338040350594\n"
     ]
    }
   ],
   "source": [
    "predictions_test_RFC = RFC_model.predict(features_test)\n",
    "mse = mean_squared_error(target_test, predictions_test_RFC)\n",
    "rmse = mse ** 0.5\n",
    "print('RMSE:', rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.776049766718507\n",
      "Precision: 0.6733333333333333\n",
      "Recall: 0.5153061224489796\n",
      "F1-Score: 0.583815028901734\n",
      "ROC AUC: 0.7028432178240424\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(target_test, predictions_test_RFC)\n",
    "precision = precision_score(target_test, predictions_test_RFC)\n",
    "recall = recall_score(target_test, predictions_test_RFC)\n",
    "f1 = f1_score(target_test, predictions_test_RFC)\n",
    "roc_auc = roc_auc_score(target_test, predictions_test_RFC)\n",
    " \n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1-Score: {f1}')\n",
    "print(f'ROC AUC: {roc_auc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of rmse is be of RMSE 0.47 indcated good fit the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the logistic regression model on the training set: 0.7422199170124482\n",
      "Accuracy of the logistic regression model on the validation set: 0.7293934681181959\n",
      "Accuracy of the logistic regression model on the test set: 0.7511664074650077\n"
     ]
    }
   ],
   "source": [
    "modelLR = LogisticRegression(\n",
    "    random_state=54321, solver=\"liblinear\"\n",
    ")\n",
    "modelLR.fit(features_train, target_train)\n",
    "score_train = modelLR.score(features_train, target_train)\n",
    "score_valid = modelLR.score(features_valid, target_valid)\n",
    "score_test = modelLR.score(features_test, target_test)\n",
    "print(\"Accuracy of the logistic regression model on the training set:\",score_train,)\n",
    "print( \"Accuracy of the logistic regression model on the validation set:\",score_valid,)\n",
    "print( \"Accuracy of the logistic regression model on the test set:\",score_test,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.49883222884552303\n"
     ]
    }
   ],
   "source": [
    "predictions_test_LR = modelLR.predict(features_test)\n",
    "mse = mean_squared_error(target_test, predictions_test_LR)\n",
    "rmse = mse ** 0.5\n",
    "print('RMSE:', rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7511664074650077\n",
      "Precision: 0.9285714285714286\n",
      "Recall: 0.1989795918367347\n",
      "F1-Score: 0.3277310924369748\n",
      "ROC AUC: 0.5961340912203807\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(target_test, predictions_test_LR)\n",
    "precision = precision_score(target_test, predictions_test_LR)\n",
    "recall = recall_score(target_test, predictions_test_LR)\n",
    "f1 = f1_score(target_test, predictions_test_LR)\n",
    "roc_auc = roc_auc_score(target_test, predictions_test_LR)\n",
    " \n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1-Score: {f1}')\n",
    "print(f'ROC AUC: {roc_auc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below show the summary result arter predict the information with sundataste Test for each model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Model result comparison table summary**\n",
    "\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "   <td><strong>Model</strong>\n",
    "   </td>\n",
    "   <td>\n",
    "    <strong>RMSE</strong>\n",
    "   </td>\n",
    "   <td>\n",
    "    <strong>Accuracy Score</strong>\n",
    "   </td>\n",
    "   <td>\n",
    "    <strong>Precision</strong>\n",
    "   </td>\n",
    "   <td>\n",
    "    <strong>Recall</strong>\n",
    "   </td>\n",
    "   <td>\n",
    "    <strong>F1-Score</strong>\n",
    "   </td>\n",
    "   <td>\n",
    "    <strong>ROC AUC</strong>\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>Decision tree classifier\n",
    "   </td>\n",
    "   <td>\n",
    "    0.461\n",
    "   </td>\n",
    "   <td>\n",
    "    0.7869\n",
    "   </td>\n",
    "   <td>\n",
    "    0.7521\n",
    "   </td>\n",
    "   <td>\n",
    "    0.4489\n",
    "   </td>\n",
    "   <td>\n",
    "    0.5623\n",
    "   </td>\n",
    "   <td>\n",
    "    0.6920\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>Random Forest Classifier\n",
    "   </td>\n",
    "   <td>\n",
    "    0.473\n",
    "   </td>\n",
    "   <td>\n",
    "    0.7760\n",
    "   </td>\n",
    "   <td>\n",
    "    0.6733\n",
    "   </td>\n",
    "   <td>\n",
    "    0.5153\n",
    "   </td>\n",
    "   <td>\n",
    "    0.5838\n",
    "   </td>\n",
    "   <td>\n",
    "    0.7028\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>Logistic Regression\n",
    "   </td>\n",
    "   <td>\n",
    "    0.498\n",
    "   </td>\n",
    "   <td>\n",
    "    0.7511\n",
    "   </td>\n",
    "   <td>\n",
    "    0.9285\n",
    "   </td>\n",
    "   <td>\n",
    "    0.1989\n",
    "   </td>\n",
    "   <td>\n",
    "    0.3277\n",
    "   </td>\n",
    "   <td>\n",
    "    0.5961\n",
    "   </td>\n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Accuracy score** Decision tree is highest value follow by Random Forest classifier\n",
    "\n",
    "**Precision** logistic Regression got highest value, next Decision tree classifier\n",
    "\n",
    "**Recall** Random Forest classifier is highest value, follow by Decision tree classifier\n",
    "\n",
    "**F1_score** Random Forest classifier is highest value follow by Decision tree classifier\n",
    "\n",
    "**Roc AUC** Random Forest classifier is highest value follow by Decision tree classifier\n",
    "\n",
    "After evaluating the metric table above, Random Forest Classifier got better results followed by Decision tree classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Test Score results\n",
      "\n",
      "Decision tree classifier  [0.79533679 0.80829016 0.80569948 0.79220779 0.82597403] \n",
      "Random Forest Classifier  [0.79015544 0.81088083 0.79274611 0.7974026  0.8025974 ] \n",
      "Logistic Regression       [0.74611399 0.70725389 0.69948187 0.72987013 0.7038961 ]\n",
      "\n",
      " Train Score results\n",
      "\n",
      "Decision tree classifier  [0.81582361 0.80674449 0.81322957 0.81659106 0.80816591] \n",
      "Random Forest Classifier  [0.98184176 0.98638132 0.98508431 0.99027868 0.9837978 ] \n",
      "Logistic Regression       [0.7464332  0.70881971 0.70363165 0.74789371 0.70187946]\n"
     ]
    }
   ],
   "source": [
    "# Perform 5-fold cross-validation\n",
    "results_DT = cross_validate(model1, features_train, target_train, cv=5,return_train_score=True)\n",
    "results_RF = cross_validate(RFC_model, features_train, target_train, cv=5,return_train_score=True)\n",
    "results_LG = cross_validate(modelLR, features_train, target_train, cv=5,return_train_score=True)\n",
    "\n",
    "# Print the results\n",
    "print(' Test Score results')\n",
    "print('\\nDecision tree classifier ',results_DT['test_score'],'\\nRandom Forest Classifier ',results_RF['test_score'],'\\nLogistic Regression      ',results_LG['test_score'])\n",
    "print('\\n Train Score results')\n",
    "print('\\nDecision tree classifier ',results_DT['train_score'],'\\nRandom Forest Classifier ',results_RF['train_score'],'\\nLogistic Regression      ',results_LG['train_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "above show results of cross-validation for the previous models.\n",
    "\n",
    "Where conclude the test score in average better result for Decision tree classifier model follow by Random Fores Classifier.\n",
    "Train score  Random Fores is highest result "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">‚úîÔ∏è\n",
    "    \n",
    "\n",
    "__Reviewer's comment ‚Ññ4__\n",
    "\n",
    "\n",
    "Otherwise it's greatüòä. Your project is begging for github =)   \n",
    "    \n",
    "Congratulations on the successful completion of the project üòäüëç\n",
    "And I wish you success in new works üòä"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
